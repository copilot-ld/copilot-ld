<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Copilot-LD â€“ Concepts</title>
    <link rel="icon" href="favicon.svg" />
    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/npm/@picocss/pico@2/css/pico.min.css"
    />
    <link rel="stylesheet" href="assets/main.css" />
  </head>

  <body>
    <header class="container">
      <hgroup>
        <h1>
          <a href="index.html">ðŸ§¬ <mark>Copilot-LD</mark></a>
        </h1>
        <p>An intelligent agent leveraging GitHub Copilot and Linked Data</p>
      </hgroup>
      <nav>
        <ul>
          <li><a href="index.html">Home</a></li>
          <li><a href="concepts.html" class="active">Concepts</a></li>
          <li>
            <details class="dropdown">
              <summary>Docs</summary>
              <ul>
                <li><a href="architecture.html">Architecture</a></li>
                <li><a href="reference.html">Reference</a></li>
              </ul>
            </details>
          </li>
          <li>
            <details class="dropdown">
              <summary>Guides</summary>
              <ul>
                <li><a href="configuration.html">Configuration</a></li>
                <li><a href="processing.html">Processing</a></li>
                <li><a href="deployment.html">Deployment</a></li>
                <li><a href="development.html">Development</a></li>
              </ul>
            </details>
          </li>
        </ul>
      </nav>
    </header>
    <main class="container">
      <h2>Core Concepts</h2>

      <p>
        Understanding the foundational concepts behind Copilot-LD helps you make
        the most of the platform. This guide explains the "why" behind key
        architectural decisions and how they work together.
      </p>

      <aside>
        <nav>
          <h5>Contents</h5>
          <ul>
            <li><a href="#overview">What is Copilot-LD?</a></li>
            <li><a href="#technologies">Core Technologies</a></li>
            <li><a href="#capabilities">System Capabilities</a></li>
            <li><a href="#flow">Request Flow</a></li>
            <li>
              <a href="#separation"
                >Why Separate Online and Offline Processing?</a
              >
            </li>
            <li><a href="#principles">Architectural Principles</a></li>
            <li><a href="#next">Next Steps</a></li>
          </ul>
        </nav>
        <hr />
      </aside>

      <h3 id="overview">What is Copilot-LD?</h3>

      <p>
        Copilot-LD is an intelligent agent that combines GitHub Copilot's
        language models with linked data and retrieval-augmented generation
        (RAG) to provide accurate, context-aware assistance. Unlike simple
        chatbots, it understands semantic relationships in your knowledge base
        and provides responses grounded in your actual data.
      </p>

      <h3 id="technologies">Core Technologies</h3>

      <h4>Linked Data</h4>

      <p>
        Linked data provides the semantic structure that makes Copilot-LD
        uniquely accurate. Instead of treating content as plain text, the system
        understands relationships and context through HTML microdata with
        Schema.org vocabularies.
      </p>

      <p><strong>Why Linked Data?</strong></p>
      <ul>
        <li>
          <strong>Semantic Understanding</strong>: Preserves meaning and
          relationships between concepts
        </li>
        <li>
          <strong>Accurate Chunking</strong>: Content boundaries align with
          semantic units, not arbitrary character limits
        </li>
        <li>
          <strong>Rich Metadata</strong>: Every piece of content includes
          structured information about its type and purpose
        </li>
        <li>
          <strong>Standard Vocabularies</strong>: Schema.org provides
          well-defined, interoperable types
        </li>
      </ul>

      <h4>Retrieval-Augmented Generation (RAG)</h4>

      <p>
        RAG enhances language model responses by retrieving relevant context
        from your knowledge base before generating answers. This grounds
        responses in factual information rather than relying solely on the
        model's training data.
      </p>

      <p><strong>The RAG Process</strong>:</p>
      <ol>
        <li><strong>Query</strong>: User asks a question or makes a request</li>
        <li>
          <strong>Retrieve</strong>: System finds relevant content using vector
          similarity search
        </li>
        <li>
          <strong>Augment</strong>: Retrieved content is added to the
          conversation context
        </li>
        <li>
          <strong>Generate</strong>: Language model produces a response informed
          by the retrieved context
        </li>
      </ol>

      <p><strong>Why RAG?</strong></p>
      <ul>
        <li>
          <strong>Accuracy</strong>: Responses based on your actual knowledge
          base, not generic training data
        </li>
        <li>
          <strong>Up-to-date</strong>: Information reflects current content
          without retraining models
        </li>
        <li>
          <strong>Transparency</strong>: Can trace responses back to source
          documents
        </li>
        <li>
          <strong>Control</strong>: You determine what information is available
          to the system
        </li>
      </ul>

      <h4>Microservices Architecture</h4>

      <p>
        Copilot-LD is built as a collection of specialized microservices that
        communicate via gRPC. Each service has a single, well-defined
        responsibility.
      </p>

      <p><strong>Why Microservices?</strong></p>
      <ul>
        <li>
          <strong>Modularity</strong>: Services can be developed, tested, and
          deployed independently
        </li>
        <li>
          <strong>Scalability</strong>: Scale individual services based on their
          specific resource needs
        </li>
        <li>
          <strong>Maintainability</strong>: Smaller, focused codebases are
          easier to understand and modify
        </li>
        <li>
          <strong>Technology Independence</strong>: Services can use different
          technologies if needed
        </li>
        <li>
          <strong>Fault Isolation</strong>: Problems in one service don't
          cascade to others
        </li>
      </ul>

      <h4>gRPC Communication</h4>

      <p>
        Services communicate using gRPC, a high-performance RPC framework with
        Protocol Buffers for message serialization.
      </p>

      <p><strong>Why gRPC?</strong></p>
      <ul>
        <li>
          <strong>Type Safety</strong>: Protocol Buffers provide strong typing
          and schema validation
        </li>
        <li>
          <strong>Performance</strong>: Binary serialization is faster and more
          compact than JSON
        </li>
        <li>
          <strong>Code Generation</strong>: Automatically generate client and
          server code from schemas
        </li>
        <li>
          <strong>Cross-Platform</strong>: Works across different languages and
          platforms
        </li>
        <li>
          <strong>Built-in Features</strong>: Authentication, timeouts, and
          error handling included
        </li>
      </ul>

      <h3 id="capabilities">System Capabilities</h3>

      <h4>Intelligent Request Processing</h4>

      <p>
        The Agent service orchestrates request processing, making autonomous
        decisions about which tools to call and when. It doesn't follow a rigid
        workflow but adapts based on the conversation context and available
        tools.
      </p>

      <h4>Contextual Memory</h4>

      <p>
        The Memory service maintains conversation history with intelligent
        budgeting. It allocates token budgets between tools, context, and
        history to maximize relevance while respecting model limits.
      </p>

      <h4>Semantic Search</h4>

      <p>The Vector service provides dual-index search capabilities:</p>
      <ul>
        <li>
          <strong>Content Search</strong>: Find documents by their actual
          content
        </li>
        <li>
          <strong>Descriptor Search</strong>: Find documents by their purpose
          and applicability
        </li>
      </ul>

      <p>
        This dual approach enables both "what does it say?" and "what is it
        for?" queries.
      </p>

      <h4>Policy-Based Access Control</h4>

      <p>
        The Graph service enforces policy-based filtering, ensuring users only
        access resources they're authorized to see. Policies are defined
        declaratively and applied consistently across all resource access.
      </p>

      <h4>Extensible Tool System</h4>

      <p>
        The Tool service enables the agent to execute external functions. Tools
        are defined using Protocol Buffers and can be added without modifying
        core services. The agent autonomously decides when to call tools based
        on conversation context.
      </p>

      <h3 id="flow">Request Flow</h3>

      <p>
        Understanding how a request flows through the system helps clarify how
        the components work together.
      </p>

      <h4>Online Processing (Runtime)</h4>

      <ol>
        <li>
          <strong>Client Request</strong>: User sends a message through an
          extension (web interface, Teams bot, etc.)
        </li>
        <li>
          <strong>Agent Orchestration</strong>: Agent service receives the
          request and validates authentication
        </li>
        <li>
          <strong>Memory Assembly</strong>: Agent requests a memory window with
          conversation history and available tools
        </li>
        <li>
          <strong>Context Retrieval</strong>: Agent resolves resource
          identifiers to actual content, with policy filtering applied
        </li>
        <li>
          <strong>Completion Generation</strong>: Agent sends assembled context
          to LLM service for response generation
        </li>
        <li>
          <strong>Tool Execution</strong>: If the LLM decides to call tools,
          Agent executes them and continues the loop
        </li>
        <li>
          <strong>Response</strong>: Final completion is saved to memory and
          returned to the client
        </li>
      </ol>

      <p>
        This flow is <strong>sequential per request</strong> but multiple
        requests can be processed concurrently. The agent makes intelligent
        decisions at each step rather than following a rigid pipeline.
      </p>

      <h4>Offline Processing (Build Time)</h4>

      <p>
        Before the system can answer questions, knowledge must be processed into
        searchable formats:
      </p>

      <ol>
        <li>
          <strong>Resource Extraction</strong>: HTML files with microdata are
          scanned and converted to individual resource documents
        </li>
        <li>
          <strong>Descriptor Generation</strong>: LLM generates descriptions of
          each resource's purpose and applicability
        </li>
        <li>
          <strong>Embedding Creation</strong>: Both content and descriptors are
          converted to vector embeddings
        </li>
        <li>
          <strong>Index Building</strong>: Vector databases are created for fast
          similarity search
        </li>
      </ol>

      <p>
        This offline pipeline ensures runtime queries are fastâ€”no external API
        calls needed during search, just in-memory vector operations.
      </p>

      <h3 id="separation">Why Separate Online and Offline Processing?</h3>

      <p>
        Copilot-LD deliberately separates build-time processing from runtime
        operations for several important reasons:
      </p>

      <h4>Performance</h4>

      <ul>
        <li>
          <strong>No API Delays</strong>: Runtime searches use pre-computed
          embeddings, eliminating LLM API latency
        </li>
        <li>
          <strong>In-Memory Operations</strong>: Vector similarity is computed
          locally without network calls
        </li>
        <li>
          <strong>Predictable Latency</strong>: Response times are consistent
          and fast
        </li>
      </ul>

      <h4>Cost Efficiency</h4>

      <ul>
        <li>
          <strong>One-Time Embeddings</strong>: Generate embeddings once during
          processing, not on every query
        </li>
        <li>
          <strong>Batch Processing</strong>: Offline pipeline optimizes API
          calls through batching
        </li>
        <li>
          <strong>No Per-Query Costs</strong>: Vector search has zero API cost
        </li>
      </ul>

      <h4>Reliability</h4>

      <ul>
        <li>
          <strong>Offline Validation</strong>: Catch processing errors before
          deployment
        </li>
        <li>
          <strong>Reduced Dependencies</strong>: Runtime doesn't depend on
          external embedding APIs
        </li>
        <li>
          <strong>Reproducible Builds</strong>: Same input always produces same
          indexes
        </li>
      </ul>

      <h3 id="principles">Architectural Principles</h3>

      <h4>Radical Simplicity</h4>

      <p>
        Copilot-LD is built with plain JavaScript and no external dependencies
        beyond Node.js built-ins. This deliberate choice makes the system:
      </p>

      <ul>
        <li>
          <strong>Easy to Understand</strong>: No framework magic or hidden
          complexity
        </li>
        <li>
          <strong>Easy to Deploy</strong>: Minimal container size (under 10 MB)
        </li>
        <li>
          <strong>Easy to Maintain</strong>: No dependency updates or
          compatibility issues
        </li>
        <li>
          <strong>Easy to Audit</strong>: Small codebase with explicit behavior
        </li>
      </ul>

      <h4>Business Logic First</h4>

      <p>
        Core logic lives in framework-agnostic packages
        (<code>@copilot-ld/lib*</code>) that can be imported and tested
        independently. Services are thin adapters that wire packages together
        with gRPC communication.
      </p>

      <p><strong>Benefits</strong>:</p>
      <ul>
        <li>
          <strong>Testability</strong>: Business logic can be unit tested
          without service infrastructure
        </li>
        <li>
          <strong>Reusability</strong>: Same logic can power different
          interfaces (CLI tools, services, extensions)
        </li>
        <li>
          <strong>Clarity</strong>: Separation between communication (gRPC) and
          computation (business logic)
        </li>
      </ul>

      <h4>Type Safety Without TypeScript</h4>

      <p>
        Protocol Buffers provide type safety and schema validation without
        requiring TypeScript compilation. Generated JavaScript includes JSDoc
        types for IDE support while remaining simple JavaScript at runtime.
      </p>

      <h4>Security by Design</h4>

      <p>Security is built into the architecture from the start:</p>

      <ul>
        <li>
          <strong>Network Isolation</strong>: Backend services are not exposed
          externally
        </li>
        <li>
          <strong>Authenticated Communication</strong>: HMAC authentication for
          all inter-service calls
        </li>
        <li>
          <strong>Time-Limited Tokens</strong>: Short-lived authentication
          tokens prevent replay attacks
        </li>
        <li>
          <strong>Policy Enforcement</strong>: Access control applied at the
          data layer
        </li>
        <li>
          <strong>Minimal Attack Surface</strong>: Small container images with
          only essential components
        </li>
      </ul>

      <h3 id="next">Next Steps</h3>

      <p>Now that you understand the core concepts, you can:</p>

      <ul>
        <li>
          <strong><a href="architecture.html">Architecture</a></strong
          >: See how these concepts map to actual system components
        </li>
        <li>
          <strong><a href="reference.html">Reference</a></strong
          >: Deep dive into implementation details
        </li>
        <li>
          <strong><a href="configuration.html">Configuration</a></strong
          >: Set up your environment
        </li>
        <li>
          <strong><a href="processing.html">Processing</a></strong
          >: Prepare your knowledge base
        </li>
      </ul>
    </main>
    <footer class="container">
      <p>Â© D. Olsson</p>
    </footer>
  </body>
</html>
