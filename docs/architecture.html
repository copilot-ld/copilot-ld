<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Copilot-LD â€“ Architecture</title>
    <link rel="icon" href="favicon.svg" />
    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/npm/@picocss/pico@2/css/pico.classless.min.css"
    />
    <link rel="stylesheet" href="assets/main.css" />
  </head>

  <body>
    <header>
      <hgroup>
        <h1>ðŸ§¬ <mark>Copilot-LD</mark></h1>
        <p>An intelligent agent leveraging GitHub Copilot and Linked Data</p>
      </hgroup>
      <nav>
        <ul>
          <li><a href="index.html">Home</a></li>
          <li><a href="architecture.html" class="active">Architecture</a></li>
          <li><a href="configuration.html">Configuration</a></li>
          <li><a href="processing.html">Processing</a></li>
          <li><a href="deployment.html">Deployment</a></li>
          <li><a href="development.html">Development</a></li>
        </ul>
      </nav>
    </header>
    <main>
      <h2>Architecture Overview</h2>

      <p>
        Copilot-LD is an intelligent agent leveraging GitHub Copilot, linked
        data and retrieval-augmented generation.
      </p>

      <h3>System Design</h3>

      <ul>
        <li>
          <strong>gRPC Microservices</strong>: Single-responsibility services
          with gRPC communication
        </li>
        <li>
          <strong>Extensions</strong>: Plugin-based adapters for different
          applications
        </li>
        <li>
          <strong>Modularity</strong>: Framework-agnostic packages for maximum
          reusability
        </li>
        <li>
          <strong>Performance</strong>: Parallel processing and optimized vector
          operations
        </li>
      </ul>

      <h4>Communication Layer</h4>

      <ul>
        <li>
          <strong>gRPC Protocol</strong>: All inter-service communication uses
          gRPC with Protocol Buffers
        </li>
        <li>
          <strong>REST APIs</strong>: Extensions expose REST endpoints for
          external client integration
        </li>
        <li>
          <strong>Schema Definition</strong>: Authoritative protobuf schemas in
          <code>/proto</code> (copied verbatim into
          <code>/generated/proto</code> during code generation) ensure type
          safety. Optional tool schemas in <code>/tools</code>
          <strong>extend the core system</strong> with additional functionality.
          All runtime loading now reads exclusively from
          <code>/generated/proto</code> for a single source of operational
          truth.
        </li>
      </ul>

      <h4>Service Architecture</h4>

      <ul>
        <li>
          <strong>Agent Service</strong>: Central orchestrator managing request
          flow
        </li>
        <li>
          <strong>Specialized Services</strong>: Domain-specific services
          (<code>LLM</code>, <code>Memory</code>, <code>Vector</code>)
        </li>
        <li>
          <strong>Parallelism</strong>: Readiness checks run in parallel; the
          main request flow is orchestrated sequentially per request
        </li>
        <li>
          <strong>Stateless Design</strong>: Services maintain no persistent
          state
        </li>
      </ul>

      <h3>Directory Structure</h3>

      <pre>
./services/          # gRPC service implementations (custom logic only)
./extensions/        # Extensions that adapt core system to applications
./packages/          # Reusable, domain-focused logic (framework agnostic)
./scripts/           # Utility scripts & code generation
./proto/             # Authoritative protobuf source schemas (never edited in generated/)
./tools/             # Custom tools that extend the core system
./generated/         # ALL generated artifacts (proto copies, types, service bases, clients)
./data/              # Knowledge base, vectors, and resource data
  </pre
      >

      <h3>High-Level Architecture</h3>

      <pre class="mermaid">
flowchart TD
    A[Clients]
    B[Extensions]
    C[Agent service]
    D[Memory service]
    E[LLM service]
    F[Tool service]
    G[Vector service]
    H[GitHub API]
    I[LLM backend]
    J[Memory store]
    L[Content Vector Index]
    N[Descriptor Vector Index]
    O[Resource Storage]

    %% Clients communicate with Extensions over REST
    A -- REST --> B

    %% Extensions interact with the Agent via gRPC
    B -- gRPC --> C

    %% Agent service interact with backend services
    C -- gRPC --> D
    C -- gRPC --> E
    C -- gRPC --> F
    C -- gRPC --> G
    C -- REST --> H

    %% Interaction with the foundation model
    E -- REST --> I

    %% Services that interact with storage
    D -- Local I/O --> J
    G -- Local I/O --> L
    G -- Local I/O --> N
    C -- Local I/O --> O
      </pre>

      <h3>Online Sequence Diagram</h3>

      <p>
        The online sequence diagram reflects the current implementation in the
        codebase. The Agent performs operations sequentially: it validates the
        GitHub token, ensures service readiness in parallel (including Tool
        service), creates an embedding for the latest user message, queries the
        <code>content</code> vector index, appends similar identifiers to memory
        (fire-and-forget), computes a token budget (subtracting
        <code>assistant.content.tokens</code> and applying configured
        allocation), assembles a memory window using the query vector and
        budget, and then enters a tool calling loop that iteratively requests
        completions and executes tools until no more tool calls are needed. Only
        readiness checks are executed in parallel; the main request flow is
        sequential.
      </p>
      <p>
        Copilot-LD is an intelligent agent leveraging GitHub Copilot, linked
        data and retrieval-augmented generation. As of 2025-09-05 the platform
        consolidates all generated protobuf artifacts into a single
        <code>/generated</code> directory to eliminate ambiguity between source
        and generated files, simplifying extension with additional tool schemas
        that <strong>extend the core system</strong>.
      </p>
      <pre class="mermaid">
sequenceDiagram
    participant Client
    participant Extension as Extensions
    participant Agent as Agent service
    participant Memory as Memory service
    participant LLM as LLM service
    participant Tool as Tool service
    participant Vector as Vector service
    participant ContentIndex as Content Vector Index
    participant ResourceIndex as Resource Index
    participant GitHub as GitHub API

    Client->>Extension: REST request
    Extension->>Agent: RPC request (ProcessRequest)

    Note right of Agent: Ensure clients ready in parallel (LLM/Vector/Memory/Tool)
    par Parallel Readiness & Validation
        Agent->>Memory: ensureReady()
        Agent->>LLM: ensureReady()
        Agent->>Vector: ensureReady()
        Agent->>Tool: ensureReady()
    and
        Agent->>GitHub: GET /user (validate token)
        GitHub-->>Agent: 200 OK (user)
    end

    Agent->>ResourceIndex: Get/Create conversation
    Agent->>ResourceIndex: Put user message (scoped to conversation)
    Agent->>ResourceIndex: Get assistant by id (from config)
    Note right of Agent: Compute token budget (budget = config.budget.tokens - assistant.content.tokens) and derive allocation

    Agent->>LLM: CreateEmbeddings (latest user message)
    LLM-->>Agent: Embedding

    Agent->>Vector: QueryItems (index: content)
    Vector->>ContentIndex: QueryIndex (similarity search)
    ContentIndex-->>Vector: Matches
    Vector-->>Agent: Identifiers (scored)

    Agent--)Memory: Append (identifiers)
    Note right of Agent: Fire-and-forget append (no await)

    Agent->>Memory: GetWindow (for conversation, with vector/budget/allocation)
    Memory-->>Agent: Window (tools, context, history)

    Agent->>ResourceIndex: Resolve window identifiers (tools/context/history)
    ResourceIndex-->>Agent: Resources (policy-filtered)

    Note over Agent: Tool Calling Loop (max 10 iterations)
    loop Until no tool calls or max iterations
        Agent->>LLM: CreateCompletions (assistant + tasks + tools + context + history)
        LLM-->>Agent: Completion with potential tool calls
        
        alt Tool calls present
            loop For each tool call
                Agent->>Tool: ExecuteTool (tool call + github_token)
                Tool-->>Agent: Tool result message
                Note right of Agent: Add tool result to messages
            end
            Note right of Agent: Continue loop with updated messages
        else No tool calls
            Note right of Agent: Exit loop - completion ready
        end
    end

    Agent->>ResourceIndex: Put final completion message (scoped to conversation)

    Agent-->>Extension: RPC response (with conversation_id)
    Extension-->>Client: REST response
  </pre
      >

      <h4>Service Responsibilities</h4>

      <h5>Agent Service</h5>

      <p>
        Central orchestrator that coordinates all other services. Requests are
        processed sequentially as shown in the sequence diagram (readiness
        checks run in parallel). The service integrates a
        <code>ResourceIndex</code> for direct resource access with policy-based
        access control.
      </p>

      <details>
        <summary>Message assembly order and budgeting</summary>
        <ul>
          <li>
            <strong>Assembly order</strong> (passed to completions):
            <code>assistant</code> â†’ <code>tasks</code> â†’ <code>tools</code> â†’
            <code>context</code> â†’ <code>history</code>
          </li>

          <li>
            <strong>gRPC Protocol</strong>: All inter-service communication uses
            gRPC with Protocol Buffers. Services load schemas exclusively from
            <code>/generated/proto</code> produced by code generation.
          </li>
          <li>
            <strong>Budgeting</strong>: Effective token budget is computed as
            <code>max(0, config.budget.tokens - assistant.content.tokens)</code
            >, with optional allocation shaping for <code>tools</code>,
            <code>history</code>, and <code>context</code>
          </li>
          <li>
            <strong>Window inputs</strong>: Memory window selection is driven by
            the query vector, effective budget, and optional allocation ratios
          </li>
        </ul>
      </details>

      <h5>Memory Service</h5>

      <p>
        Manages transient conversation context as resource identifiers with
        JSONL (newline-delimited JSON) storage for efficient appends. Provides
        memory windows used by the Agent to assemble context.
      </p>

      <h5>LLM Service</h5>

      <p>
        Interfaces with language models for embedding generation and text
        completion. Handles communication with external AI services.
      </p>

      <h5>Vector Service</h5>

      <p>
        Performs similarity search operations against dual vector indexes
        (content and descriptor). Returns resource identifiers and similarity
        scores ordered by relevance, with support for index type selection and
        token-based filtering. The current Agent runtime path queries the
        <code>content</code> index by default.
      </p>

      <h5>Tool Service</h5>

      <p>
        Acts as a gRPC proxy between tool calls requested by LLMs and actual
        tool implementations. Supports both mapping to existing services and
        custom tool implementations that
        <strong>extend the core system</strong> through configuration-driven
        endpoint mapping.
      </p>

      <p><strong>Key Operations</strong>:</p>
      <ul>
        <li>
          <code>ExecuteTool</code>: Proxies tool calls to appropriate services
          based on configuration mapping
        </li>
        <li>
          <code>ListTools</code>: Returns available tools with OpenAI-compatible
          schemas for LLM consumption
        </li>
      </ul>

      <p><strong>Architecture</strong>:</p>
      <ul>
        <li>
          <strong>Configuration-driven</strong>: Tools defined via endpoint
          mappings in <code>config.yml</code>
        </li>
        <li>
          <strong>Pure proxy</strong>: No business logic, just routing and
          protocol conversion
        </li>
        <li>
          <strong>Extensible</strong>: Supports mapping to existing services
          (e.g., <code>vector.QueryItems</code>) and custom tools that
          <strong>extend the platform</strong>
        </li>
        <li>
          <strong>Schema generation</strong>: Automatically generates
          OpenAI-compatible JSON schemas from protobuf types
        </li>
      </ul>

      <h3>Offline Sequence Diagram</h3>

      <p>
        The platform includes offline scripts for knowledge base preparation and
        vector index creation. These scripts process external knowledge sources
        into searchable vector indices before the services are deployed.
      </p>

      <pre class="mermaid">
sequenceDiagram
    participant Dev as Developer
    participant Download as scripts/download.js
    participant GitHub as GitHub API
    participant Chunk as scripts/chunk.js
    participant Index as scripts/index.js
    participant LLM as LLM API
    participant Storage as Local Storage

    Dev->>Download: npm run download
    Download->>GitHub: Fetch latest release artifacts
    GitHub-->>Download: Release assets (.tar.gz)
    Download->>Storage: Extract to data/knowledge/
    Note right of Download: HTML files with microdata

    Dev->>Resources: node scripts/resources.js
    Resources->>Storage: Read HTML files from data/knowledge/
    Storage-->>Resources: HTML content with microdata
    loop For each HTML file
        Resources->>Resources: Extract microdata items as resources
        Resources->>Resources: Generate resource identifier (URI)
        Resources->>Resources: Apply policy filtering
        Resources->>Storage: Store resource.json with metadata
    end
    Resources->>Storage: Persist resource index

    Dev->>Index: node scripts/index.js
    Index->>Storage: Load resource index
    Storage-->>Index: All resource metadata

    loop Process resources in batches (content & descriptor)
        Index->>Storage: Load resource content
        Storage-->>Index: Resource content/descriptors
        Index->>LLM: Create embeddings for resource batch
        LLM-->>Index: Resource embeddings
        Index->>Storage: Add to dual vector indexes
    end

    Index->>Storage: Persist content & descriptor vector indexes
    Note right of Storage: Ready for runtime vector search with dual indexes
      </pre>

      <h4>Offline Processing Workflow</h4>

      <h5>1. Knowledge Download (<code>scripts/download.js</code>)</h5>

      <ul>
        <li>Downloads latest release artifacts from GitHub repository</li>
        <li>
          Extracts compressed archives to <code>data/knowledge/</code> directory
        </li>
        <li>Provides HTML files with structured microdata for processing</li>
      </ul>

      <h5>2. Resource Processing (<code>scripts/resources.js</code>)</h5>

      <ul>
        <li>
          Scans HTML files for microdata items with configurable selectors
        </li>
        <li>
          Extracts structured content and generates unique resource identifiers
          using URI format
        </li>
        <li>Applies policy-based filtering for access control</li>
        <li>
          Stores individual resources with metadata in unified resource storage
        </li>
        <li>Creates searchable resource index with type-based organization</li>
      </ul>

      <h5>3. Vector Indexing (<code>scripts/index.js</code>)</h5>

      <ul>
        <li>
          <strong>Dual-Index Architecture</strong>: Creates separate content and
          descriptor vector indexes
        </li>
        <li>
          <strong>Batch Processing</strong>: Processes resources in
          token-optimized batches to minimize API calls
        </li>
        <li>
          <strong>Embedding Generation</strong>: Creates vector embeddings for
          both content and descriptors via LLM API
        </li>
        <li>
          <strong>Index Creation</strong>: Builds dual indexes for flexible
          search capabilities
        </li>
        <li>
          <strong>Persistence</strong>: Stores indexes to disk for runtime
          access with index type selection
        </li>
      </ul>

      <h5>Key Characteristics</h5>

      <ul>
        <li>
          <strong>Offline Execution</strong>: All processing occurs before
          service deployment
        </li>
        <li>
          <strong>API Optimization</strong>: Batched requests minimize LLM API
          calls and costs
        </li>
        <li>
          <strong>Resource-Based Architecture</strong>: Unified resource
          management with policy control
        </li>
        <li>
          <strong>Flexible Search</strong>: Dual-index system enables content
          and descriptor-based searches
        </li>
        <li>
          <strong>Incremental Processing</strong>: Skips existing resources to
          support iterative updates
        </li>
        <li>
          <strong>Token Management</strong>: Respects API token limits while
          maximizing batch efficiency
        </li>
      </ul>

      <h5>Data Flow</h5>

      <ol>
        <li><strong>Raw Knowledge</strong> â†’ HTML files with microdata</li>
        <li>
          <strong>Structured Resources</strong> â†’ Individual JSON files with
          metadata and URI identifiers
        </li>
        <li>
          <strong>Vector Embeddings</strong> â†’ Numerical representations for
          similarity search (content & descriptors)
        </li>
        <li>
          <strong>Dual Vector Indexes</strong> â†’ Content and descriptor
          databases ready for runtime queries with index type selection
        </li>
      </ol>

      <p>
        This offline pipeline ensures that runtime services can perform fast
        vector similarity searches without depending on external APIs or
        requiring real-time embedding generation.
      </p>

      <h3 id="code-generation">Code Generation</h3>

      <p>
        All operational protobuf schemas and generated code live exclusively
        under
        <code>generated/</code> to eliminate ambiguity about which artifacts are
        source vs generated. The <code>@copilot-ld/libutil</code>
        <code>Codegen</code> class performs these steps:
      </p>

      <ol>
        <li>
          Discovers all <code>.proto</code> files in <code>proto/</code> (and
          <strong
            >optional tool proto files in <code>tools/</code> that extend the
            platform</strong
          >)
        </li>
        <li>Copies them into <code>packages/libtype/generated/proto/</code></li>
        <li>
          Generates consolidated JavaScript types in
          <code>packages/libtype/generated/types.js</code> and TypeScript
          declarations in <code>types.d.ts</code>
        </li>
        <li>
          Generates service base classes into
          <code
            >packages/librpc/generated/services/&lt;name&gt;/service.js</code
          >
        </li>
        <li>
          Generates client classes into
          <code>packages/librpc/generated/services/&lt;name&gt;/client.js</code>
        </li>
        <li>
          Generates dynamic exports aggregating all services and clients into
          <code>packages/librpc/generated/services/exports.js</code>
        </li>
      </ol>

      <p>
        <strong>Package Architecture:</strong> Generated code is distributed
        across two packages for clean separation:
        <code>@copilot-ld/libtype</code> contains pure protobuf type
        definitions, while <code>@copilot-ld/librpc</code> contains RPC
        infrastructure (service bases, clients, networking). This prevents
        circular dependencies and ensures services import types from libtype and
        applications import service/client classes from librpc.
      </p>

      <p>
        You should re-run code generation after changing any schema in
        <code>proto/</code> or adding a new tool schema in
        <code>tools/</code> (<strong>tools extend the core system</strong>):
      </p>

      <pre><code># Generate everything (types + service bases + clients)
npm run codegen

# Or run a specific generator (invokes @copilot-ld/libutil codegen utility)
npm run codegen:type     # Generate consolidated types into packages/libtype/generated/
npm run codegen:service  # Generate base classes into packages/librpc/generated/
npm run codegen:client   # Generate typed clients into packages/librpc/generated/</code></pre>

      <p>
        These scripts use the <code>@copilot-ld/libutil</code>
        <code>Codegen</code> class (available via <code>npx codegen</code>)
        which uses <code>protobufjs</code> and <code>Mustache</code> templates
        to produce ESM modules. Generated artifacts are placed in
        package-specific <code>generated/</code> directories. Runtime loading of
        protobuf schemas reads from
        <code>packages/libtype/generated/proto/</code>.
      </p>

      <h4>Generated Artifact Structure</h4>

      <pre>
./packages/libtype/generated/
â”œâ”€â”€ proto/           # Copied original .proto files
â”œâ”€â”€ types.js         # Consolidated static types
â””â”€â”€ types.d.ts       # TypeScript declarations

./packages/librpc/generated/
â””â”€â”€ services/        # Service base classes and clients
    â”œâ”€â”€ agent/
    â”‚   â”œâ”€â”€ service.js   # Generated base class
    â”‚   â””â”€â”€ client.js    # Generated typed client
    â”œâ”€â”€ exports.js       # Aggregated service/client exports
    â””â”€â”€ .../
      </pre>

      <h4>Code Generation Workflow</h4>

      <p>
        The code generation process ensures type safety and consistency across
        the entire platform:
      </p>

      <ul>
        <li>
          <strong>Schema Discovery</strong>: Automatically finds all
          <code>.proto</code> files in source directories
        </li>
        <li>
          <strong>Type Generation</strong>: Creates unified type definitions
          accessible via <code>@copilot-ld/libtype</code>
        </li>
        <li>
          <strong>Service Generation</strong>: Produces base classes that
          services extend with business logic
        </li>
        <li>
          <strong>Client Generation</strong>: Creates typed gRPC clients for
          inter-service communication
        </li>
        <li>
          <strong>Tool Extension</strong>: Supports additional tools that extend
          the core system functionality
        </li>
      </ul>

      <h3>Security Architecture</h3>

      <p>
        The security design focuses on service authentication and secure
        communication channels.
      </p>

      <h4>Authentication Mechanisms</h4>

      <h5>Current State</h5>

      <ul>
        <li>
          <strong>Service-to-Service</strong>: HMAC-SHA256 authentication is
          implemented and enforced between internal services via the
          <code>@copilot-ld/librpc</code> package. All gRPC communication uses
          time-limited tokens with Bearer authentication headers.
        </li>
        <li>
          <strong>Request-Level</strong>: The Agent validates incoming
          <code>github_token</code> by calling <code>GET /user</code> via
          GitHub's API before processing requests.
        </li>
      </ul>

      <h5>HMAC Service Authentication (Current Implementation)</h5>

      <p>
        The platform implements HMAC-SHA256 authentication for all
        service-to-service communication:
      </p>

      <pre class="mermaid">
sequenceDiagram
    participant Service A
    participant Service B
    participant Authenticator

    Service A->>Authenticator: generateToken(serviceId)
    Authenticator->>Authenticator: Create payload: serviceId:timestamp
    Authenticator->>Authenticator: Sign with HMAC-SHA256
    Authenticator-->>Service A: Base64 encoded token

    Service A->>Service B: gRPC request + token
    Service B->>Authenticator: verifyToken(token)
    Authenticator->>Authenticator: Decode and validate signature
    Authenticator->>Authenticator: Check token expiration
    Authenticator-->>Service B: {isValid, serviceId, error}

    alt Token Valid
        Service B-->>Service A: Process request
    else Token Invalid
        Service B-->>Service A: Authentication error
    end
      </pre>

      <h5>Token Generation Process (Implemented)</h5>

      <ol>
        <li>Create payload combining service ID and current timestamp</li>
        <li>
          Generate HMAC-SHA256 signature using shared secret from
          <code>SERVICE_AUTH_SECRET</code>
        </li>
        <li>
          Encode as Base64: <code>Base64(serviceId:timestamp:signature)</code>
        </li>
        <li>
          Attach token to gRPC metadata as
          <code>Authorization: Bearer &lt;token&gt;</code>
        </li>
      </ol>

      <h5>Token Verification Process (Implemented)</h5>

      <ol>
        <li>
          Extract token from <code>Authorization</code> header in gRPC metadata
        </li>
        <li>
          Decode Base64 token and split into service ID, timestamp, and
          signature
        </li>
        <li>
          Verify timestamp is within 60-second token lifetime (configurable)
        </li>
        <li>Recreate expected signature using shared secret</li>
        <li>Compare signatures using secure comparison</li>
        <li>
          Reject request with <code>UNAUTHENTICATED</code> status if validation
          fails
        </li>
      </ol>

      <h4>Communication Security</h4>

      <h5>gRPC Internal Communication</h5>

      <ul>
        <li><strong>Protocol</strong>: gRPC over HTTP/2</li>
        <li>
          <strong>Authentication</strong>: HMAC-SHA256 with time-limited Bearer
          tokens
        </li>
        <li><strong>Token Lifetime</strong>: 60 seconds (configurable)</li>
        <li>
          <strong>Secret Management</strong>: Shared
          <code>SERVICE_AUTH_SECRET</code> environment variable
        </li>
        <li>
          <strong>Schema Validation</strong>: Protocol Buffer message validation
        </li>
      </ul>

      <h5>External API Communication</h5>

      <ul>
        <li><strong>Clients to Extensions</strong>: HTTP/HTTPS via the ALB</li>
        <li>
          <strong>LLM Service to External APIs</strong>: HTTPS with API key
          authentication
        </li>
      </ul>

      <h4>Security Limitations</h4>

      <h5>mTLS Not Implemented</h5>

      <p>
        Mutual TLS (mTLS) is not currently implemented between services. Future
        security enhancements should include:
      </p>

      <ul>
        <li>Certificate-based service authentication</li>
        <li>Encrypted gRPC communication channels</li>
        <li>Service identity verification via X.509 certificates</li>
      </ul>

      <h5>Rate-Limiting Not Implemented</h5>

      <p>
        Rate-limiting is not currently implemented for externally facing
        services. Future enhancements should include:
      </p>

      <ul>
        <li>Request throttling per client IP address to prevent abuse</li>
        <li>Adaptive rate limiting based on service resource utilization</li>
        <li>
          Token bucket or sliding window algorithms for burst traffic handling
        </li>
        <li>
          Configurable rate limits per extension type (web vs API clients)
        </li>
      </ul>

      <h4>Threat Model</h4>

      <h5>Protected Against</h5>

      <ol>
        <li>
          <strong>External Service Access</strong>: Backend services cannot be
          directly accessed from outside the Docker environment
        </li>
        <li>
          <strong>Service Impersonation</strong>: HMAC authentication prevents
          unauthorized service access with cryptographic verification
        </li>
        <li>
          <strong>Token Replay</strong>: 60-second time-limited tokens minimize
          replay attack windows
        </li>
        <li>
          <strong>Request Forgery</strong>: HMAC signatures prevent message
          tampering and ensure request authenticity
        </li>
      </ol>

      <h5>Current Vulnerabilities</h5>

      <ol>
        <li>
          <strong>Shared Secret Exposure</strong>: If
          <code>SERVICE_AUTH_SECRET</code> is compromised, all service
          authentication is bypassed
        </li>
        <li>
          <strong>Container Compromise</strong>: If one container is
          compromised, the attacker gains access to the shared authentication
          secret
        </li>
        <li>
          <strong>Extension Security</strong>: Extensions are the primary attack
          surface and must implement their own input validation
        </li>
      </ol>

      <h4>Service Security Responsibilities</h4>

      <h5>Extensions (Web, Copilot)</h5>

      <ul>
        <li>Input validation and sanitization</li>
        <li>Rate limiting and DDoS protection</li>
        <li>Session management</li>
        <li>CORS policy enforcement</li>
      </ul>

      <h5>Agent Service</h5>

      <ul>
        <li>Request orchestration security</li>
        <li>Service-to-service authentication enforcement</li>
        <li>Business logic security validation</li>
      </ul>

      <h5>Backend Services (Memory, LLM, Vector)</h5>

      <ul>
        <li>gRPC message validation</li>
        <li>Resource usage limiting</li>
        <li>Data access controls</li>
        <li>Error handling without information disclosure</li>
      </ul>
    </main>
    <footer>
      <p>Â© D. Olsson</p>
    </footer>

    <script type="module">
      import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11/dist/mermaid.esm.min.mjs";
    </script>
  </body>
</html>
